{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b443796",
   "metadata": {},
   "source": [
    "# Stage 1 - Make and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.319804</td>\n",
       "      <td>3.377585</td>\n",
       "      <td>336.459274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.131497</td>\n",
       "      <td>573.395977</td>\n",
       "      <td>466.857149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-999.540014</td>\n",
       "      <td>-999.991621</td>\n",
       "      <td>-970.509688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-502.060301</td>\n",
       "      <td>-485.131598</td>\n",
       "      <td>7.165847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-7.168021</td>\n",
       "      <td>4.138413</td>\n",
       "      <td>414.690881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>497.127856</td>\n",
       "      <td>497.560860</td>\n",
       "      <td>730.794642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.305530</td>\n",
       "      <td>999.432485</td>\n",
       "      <td>999.432485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y           max\n",
       "count  10000.000000  10000.000000  10000.000000\n",
       "mean      -3.319804      3.377585    336.459274\n",
       "std      577.131497    573.395977    466.857149\n",
       "min     -999.540014   -999.991621   -970.509688\n",
       "25%     -502.060301   -485.131598      7.165847\n",
       "50%       -7.168021      4.138413    414.690881\n",
       "75%      497.127856    497.560860    730.794642\n",
       "max      999.305530    999.432485    999.432485"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "## dataset generation\n",
    "# lets say we want to train a function that returns max(x,y)\n",
    "\n",
    "input_data = pd.DataFrame({\n",
    "    'x': np.random.uniform(-1000, 1000, 10000),\n",
    "    'y': np.random.uniform(-1000, 1000, 10000)\n",
    "})\n",
    "output_data = pd.DataFrame({\n",
    "    'output': np.maximum(input_data['x'], input_data['y'])\n",
    "})\n",
    "\n",
    "\n",
    "df = pd.concat([input_data, output_data], axis=1)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.027113676071167\n",
      "Epoch 1, Loss: 0.6868435144424438\n",
      "Epoch 2, Loss: 0.5107424855232239\n",
      "Epoch 3, Loss: 0.5535025596618652\n",
      "Epoch 4, Loss: 0.5595170855522156\n",
      "Epoch 5, Loss: 0.45030251145362854\n",
      "Epoch 6, Loss: 0.37930625677108765\n",
      "Epoch 7, Loss: 0.5606532096862793\n",
      "Epoch 8, Loss: 0.7571465969085693\n",
      "Epoch 9, Loss: 0.363572359085083\n",
      "Epoch 10, Loss: 0.510280966758728\n",
      "Epoch 11, Loss: 0.3281683921813965\n",
      "Epoch 12, Loss: 0.21552875638008118\n",
      "Epoch 13, Loss: 0.264011949300766\n",
      "Epoch 14, Loss: 0.39230236411094666\n",
      "Epoch 15, Loss: 1.6711008548736572\n",
      "Epoch 16, Loss: 0.7677030563354492\n",
      "Epoch 17, Loss: 0.10186141729354858\n",
      "Epoch 18, Loss: 1.390590786933899\n",
      "Epoch 19, Loss: 1.641930103302002\n",
      "Epoch 20, Loss: 0.006918319500982761\n",
      "Epoch 21, Loss: 0.29741910099983215\n",
      "Epoch 22, Loss: 9.805606842041016\n",
      "Epoch 23, Loss: 0.05324534326791763\n",
      "Epoch 24, Loss: 0.2600162923336029\n",
      "Epoch 25, Loss: 0.03549977019429207\n",
      "Epoch 26, Loss: 4.396921157836914\n",
      "Epoch 27, Loss: 0.047415345907211304\n",
      "Epoch 28, Loss: 0.06699024140834808\n",
      "Epoch 29, Loss: 0.011226747184991837\n",
      "Epoch 30, Loss: 0.024132385849952698\n",
      "Epoch 31, Loss: 3.0141680240631104\n",
      "Epoch 32, Loss: 0.012227760627865791\n",
      "Epoch 33, Loss: 0.06097542867064476\n",
      "Epoch 34, Loss: 2.7090964317321777\n",
      "Epoch 35, Loss: 0.04190286248922348\n",
      "Epoch 36, Loss: 0.0016599581576883793\n",
      "Epoch 37, Loss: 0.2932332754135132\n",
      "Epoch 38, Loss: 2.6188483238220215\n",
      "Epoch 39, Loss: 0.005331520922482014\n",
      "Epoch 40, Loss: 0.18916268646717072\n",
      "Epoch 41, Loss: 1.200317621231079\n",
      "Epoch 42, Loss: 0.02876601554453373\n",
      "Epoch 43, Loss: 0.05534853786230087\n",
      "Epoch 44, Loss: 0.013746258802711964\n",
      "Epoch 45, Loss: 3.7690563201904297\n",
      "Epoch 46, Loss: 0.0033401083201169968\n",
      "Epoch 47, Loss: 0.00927702896296978\n",
      "Epoch 48, Loss: 0.001969586592167616\n",
      "Epoch 49, Loss: 7.892255783081055\n",
      "Epoch 50, Loss: 0.05180463194847107\n",
      "Epoch 51, Loss: 0.2616443336009979\n",
      "Epoch 52, Loss: 0.23132890462875366\n",
      "Epoch 53, Loss: 0.2218291312456131\n",
      "Epoch 54, Loss: 1.925676703453064\n",
      "Epoch 55, Loss: 0.020936859771609306\n",
      "Epoch 56, Loss: 0.029791884124279022\n",
      "Epoch 57, Loss: 0.6097409129142761\n",
      "Epoch 58, Loss: 3.877697229385376\n",
      "Epoch 59, Loss: 0.009130577556788921\n",
      "Epoch 60, Loss: 0.7353149652481079\n",
      "Epoch 61, Loss: 3.8362059593200684\n",
      "Epoch 62, Loss: 1.8747286796569824\n",
      "Epoch 63, Loss: 0.0009548831731081009\n",
      "Epoch 64, Loss: 3.1388072967529297\n",
      "Epoch 65, Loss: 0.02200288325548172\n",
      "Epoch 66, Loss: 0.0010711662471294403\n",
      "Epoch 67, Loss: 0.781541645526886\n",
      "Epoch 68, Loss: 0.11190729588270187\n",
      "Epoch 69, Loss: 0.07801464945077896\n",
      "Epoch 70, Loss: 0.966943621635437\n",
      "Epoch 71, Loss: 0.8915350437164307\n",
      "Epoch 72, Loss: 3.956479549407959\n",
      "Epoch 73, Loss: 0.010926961898803711\n",
      "Epoch 74, Loss: 0.0010149634908884764\n",
      "Epoch 75, Loss: 0.11199354380369186\n",
      "Epoch 76, Loss: 0.2167947143316269\n",
      "Epoch 77, Loss: 0.004634004086256027\n",
      "Epoch 78, Loss: 6.03225040435791\n",
      "Epoch 79, Loss: 0.015500523149967194\n",
      "Epoch 80, Loss: 0.054158590734004974\n",
      "Epoch 81, Loss: 0.0033023026771843433\n",
      "Epoch 82, Loss: 0.7716822624206543\n",
      "Epoch 83, Loss: 0.23393577337265015\n",
      "Epoch 84, Loss: 0.038406506180763245\n",
      "Epoch 85, Loss: 0.010238857008516788\n",
      "Epoch 86, Loss: 0.30118149518966675\n",
      "Epoch 87, Loss: 25.607234954833984\n",
      "Epoch 88, Loss: 0.03504207357764244\n",
      "Epoch 89, Loss: 0.15657618641853333\n",
      "Epoch 90, Loss: 0.0031525662634521723\n",
      "Epoch 91, Loss: 35.011680603027344\n",
      "Epoch 92, Loss: 0.02024904265999794\n",
      "Epoch 93, Loss: 0.04339053854346275\n",
      "Epoch 94, Loss: 0.9275851249694824\n",
      "Epoch 95, Loss: 1.375625729560852\n",
      "Epoch 96, Loss: 0.00455007329583168\n",
      "Epoch 97, Loss: 2.7735958099365234\n",
      "Epoch 98, Loss: 1.8003697395324707\n",
      "Epoch 99, Loss: 0.059104591608047485\n"
     ]
    }
   ],
   "source": [
    "# model definition and training\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(2, 10)\n",
    "        self.linear2 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    # this method is necessary for the model to be evaluated by poly-lithic\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        # x will be a dicrt of keys and values\n",
    "        # {\"x\": x, \"y\": y}\n",
    "        input_tensor = torch.tensor([x['x'], x['y']], dtype=torch.float32)\n",
    "        # you may want to do somethinf more complex here\n",
    "        output_tensor = self.forward(input_tensor)\n",
    "        # return a dictionary of keys and values\n",
    "        return {\"output\": output_tensor.item()}        \n",
    "    \n",
    "# model training\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "batch_size = 32\n",
    "\n",
    "# convert data to torch tensors\n",
    "X = torch.tensor(df[['x', 'y']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df[['output']].values, dtype=torch.float32)\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    \n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'local/model.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fedb6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4179.904375\n",
      "Input: [ 573.66538305 -497.08576156], Predicted: 573.4780883789062, Actual: 573.6654052734375\n",
      "Input: [-755.38952928  165.59900853], Predicted: 165.28440856933594, Actual: 165.59901428222656\n",
      "Input: [-728.08504207  197.48211559], Predicted: 197.1942901611328, Actual: 197.48211669921875\n",
      "Input: [355.55092175 976.00125353], Predicted: 975.845703125, Actual: 976.0012817382812\n",
      "Input: [-386.99307363 -953.92977677], Predicted: -387.9506530761719, Actual: -386.9930725097656\n",
      "Input: [ 562.55284657 -884.40475181], Predicted: 562.03857421875, Actual: 562.5528564453125\n",
      "Input: [-733.48783562 -964.21029821], Predicted: -734.5945434570312, Actual: -733.4878540039062\n",
      "Input: [418.205749   -30.50748379], Predicted: 418.2569580078125, Actual: 418.20574951171875\n",
      "Input: [254.79010163 291.42216212], Predicted: 291.3809509277344, Actual: 291.4221496582031\n",
      "Input: [-934.28593574  474.82438214], Predicted: 474.5726013183594, Actual: 474.8243713378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbm96348/nfs_home/ml_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_data = pd.DataFrame({\n",
    "    'x': np.random.uniform(-1000, 1000, 100),\n",
    "    'y': np.random.uniform(-1000, 1000, 100)\n",
    "})\n",
    "test_X = torch.tensor(test_data[['x', 'y']].values, dtype=torch.float32)\n",
    "test_y = torch.tensor(np.maximum(test_data['x'], test_data['y']), dtype=torch.float32)\n",
    "test_y_pred = model(test_X)\n",
    "test_loss = loss_fn(test_y_pred, test_y)\n",
    "print(f'Test Loss: {test_loss.item()/len(test_X)}')\n",
    "\n",
    "# print sample\n",
    "for i in range(10):\n",
    "    print(f\"Input: {test_data.iloc[i].values}, Predicted: {test_y_pred[i].item()}, Actual: {test_y[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b5966",
   "metadata": {},
   "source": [
    "# Stage 2a - Write a poly-lithic compatible model  - from local file system\n",
    "\n",
    "Look inside `examples/base/local/model_definition.py`\n",
    "\n",
    "The file is composed of a model factory and a model definition. Which we point to in the deployment.yaml file.\n",
    "\n",
    "\n",
    "Model definition as beofre:\n",
    "\n",
    "``` python\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(2, 10)\n",
    "        self.linear2 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    # this method is necessary for the model to be evaluated by poly-lithic\n",
    "    \n",
    "    def evaluate(self, x: dict) -> dict:\n",
    "        # x will be a dicrt of keys and values\n",
    "        # {\"x\": x, \"y\": y}\n",
    "        input_tensor = torch.tensor([x['x'], x['y']], dtype=torch.float32)\n",
    "        # you may want to do somethinf more complex here\n",
    "        output_tensor = self.forward(input_tensor)\n",
    "        # return a dictionary of keys and values\n",
    "        return {\"output\": output_tensor.item()}\n",
    "```\n",
    "\n",
    "Model factory:\n",
    "\n",
    "``` python\n",
    "class ModelFactory:\n",
    "    \n",
    "    # can do more complex things here but we will just load the model from a locally saved file\n",
    "    def __init__(self):\n",
    "        self.model = SimpleModel()\n",
    "        self.model = self.model.load_state_dict(torch.load('model.pth'))\n",
    "        # other magic init stuff here\n",
    "        print(\"ModelFactory initialized\")\n",
    "    \n",
    "    # this method is necessary for the model to be retrieved by poly-lithic\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9b9cc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "ModelFactory initialized\n",
      "Input: {'x': -20, 'y': 1000.0}, Output: {'max': 999.8095092773438}\n"
     ]
    }
   ],
   "source": [
    "# lets check our import works\n",
    "from local.model_definition import ModelFactory\n",
    "\n",
    "factory = ModelFactory()\n",
    "\n",
    "model = factory.get_model()\n",
    "\n",
    "\n",
    "sample_input = {\n",
    "    \"x\": -20,\n",
    "    \"y\": 1000.0\n",
    "}\n",
    "sample_output = model.evaluate(sample_input)\n",
    "print(f\"Input: {sample_input}, Output: {sample_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad617072",
   "metadata": {},
   "source": [
    "In this case our `deployment_config.yaml` file will look like this:\n",
    "\n",
    "``` yaml\n",
    "deployment:\n",
    "  type: \"continuous\"\n",
    "  # other configurations\n",
    "input_data:\n",
    "  get_method: \"p4p_server\"\n",
    "  config:\n",
    "    # EPICS_PVA_NAME_SERVERS: \"localhost:5075\"\n",
    "    # intialize: false\n",
    "    variables:\n",
    "      LUME:MLFLOW:TEST_B:\n",
    "        proto: pva\n",
    "        name: LUME:MLFLOW:TEST_B\n",
    "      LUME:MLFLOW:TEST_A:\n",
    "        proto: pva\n",
    "        name: LUME:MLFLOW:TEST_A\n",
    "\n",
    "input_data_to_model:\n",
    "  type: \"SimpleTransformer\"\n",
    "  config:\n",
    "    symbols:\n",
    "      - \"LUME:MLFLOW:TEST_B\"\n",
    "      - \"LUME:MLFLOW:TEST_A\"\n",
    "    variables:\n",
    "      x2:\n",
    "        formula: \"LUME:MLFLOW:TEST_B\"\n",
    "      x1: \n",
    "        formula: \"LUME:MLFLOW:TEST_A\"\n",
    "\n",
    "outputs_model:\n",
    "  config:\n",
    "    variables:\n",
    "      y:\n",
    "        type: \"scalar\"\n",
    "\n",
    "output_model_to_data:\n",
    "  type: \"SimpleTransformer\"\n",
    "  config:\n",
    "    symbols:\n",
    "      - \"y\"\n",
    "    variables:\n",
    "      LUME:MLFLOW:TEST_G:\n",
    "        formula: \"y\"\n",
    "\n",
    "output_data_to:\n",
    "  put_method: \"p4p_server\"\n",
    "  config:\n",
    "    variables:\n",
    "      LUME:MLFLOW:TEST_G:\n",
    "        proto: pva\n",
    "        name: LUME:MLFLOW:TEST_G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca2855",
   "metadata": {},
   "source": [
    "# Stage 2b - Write a poly-lithic compatible model  - from mlflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

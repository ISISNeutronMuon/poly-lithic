{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b443796",
   "metadata": {},
   "source": [
    "# Stage 1 - Make and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b57419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-21.800063</td>\n",
       "      <td>-0.506054</td>\n",
       "      <td>324.312501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>578.818310</td>\n",
       "      <td>578.531328</td>\n",
       "      <td>474.106109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-999.961736</td>\n",
       "      <td>-999.873428</td>\n",
       "      <td>-987.980280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-527.600375</td>\n",
       "      <td>-504.048709</td>\n",
       "      <td>-10.528082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-34.174098</td>\n",
       "      <td>0.665823</td>\n",
       "      <td>399.594383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>470.701620</td>\n",
       "      <td>499.911204</td>\n",
       "      <td>729.902342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.789837</td>\n",
       "      <td>999.878718</td>\n",
       "      <td>999.878718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y           max\n",
       "count  10000.000000  10000.000000  10000.000000\n",
       "mean     -21.800063     -0.506054    324.312501\n",
       "std      578.818310    578.531328    474.106109\n",
       "min     -999.961736   -999.873428   -987.980280\n",
       "25%     -527.600375   -504.048709    -10.528082\n",
       "50%      -34.174098      0.665823    399.594383\n",
       "75%      470.701620    499.911204    729.902342\n",
       "max      999.789837    999.878718    999.878718"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "## dataset generation\n",
    "# lets say we want to train a function that returns max(x,y)\n",
    "\n",
    "input_data = pd.DataFrame({\n",
    "    'x': np.random.uniform(-1000, 1000, 10000),\n",
    "    'y': np.random.uniform(-1000, 1000, 10000)\n",
    "})\n",
    "output_data = pd.DataFrame({\n",
    "    'max': np.maximum(input_data['x'], input_data['y'])\n",
    "})\n",
    "\n",
    "\n",
    "df = pd.concat([input_data, output_data], axis=1)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1bd9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 192.95655822753906\n",
      "Epoch 1, Loss: 25.026445388793945\n",
      "Epoch 2, Loss: 10.21835708618164\n",
      "Epoch 3, Loss: 7.42478609085083\n",
      "Epoch 4, Loss: 5.154231071472168\n",
      "Epoch 5, Loss: 2.9389894008636475\n",
      "Epoch 6, Loss: 1.4529979228973389\n",
      "Epoch 7, Loss: 0.7924959659576416\n",
      "Epoch 8, Loss: 0.4496413767337799\n",
      "Epoch 9, Loss: 0.2790074646472931\n",
      "Epoch 10, Loss: 0.15671026706695557\n",
      "Epoch 11, Loss: 0.08996041864156723\n",
      "Epoch 12, Loss: 0.04478834569454193\n",
      "Epoch 13, Loss: 0.014473375864326954\n",
      "Epoch 14, Loss: 0.007029773201793432\n",
      "Epoch 15, Loss: 0.004069437272846699\n",
      "Epoch 16, Loss: 0.012737715616822243\n",
      "Epoch 17, Loss: 0.003976777195930481\n",
      "Epoch 18, Loss: 0.032771702855825424\n",
      "Epoch 19, Loss: 0.16918373107910156\n",
      "Epoch 20, Loss: 0.4616890847682953\n",
      "Epoch 21, Loss: 2.6224358081817627\n",
      "Epoch 22, Loss: 8.3590087890625\n",
      "Epoch 23, Loss: 0.04710977524518967\n",
      "Epoch 24, Loss: 18.396827697753906\n",
      "Epoch 25, Loss: 2.970618486404419\n",
      "Epoch 26, Loss: 1.8113993406295776\n",
      "Epoch 27, Loss: 0.017150355502963066\n",
      "Epoch 28, Loss: 0.01350686140358448\n",
      "Epoch 29, Loss: 13.226421356201172\n",
      "Epoch 30, Loss: 0.1760527789592743\n",
      "Epoch 31, Loss: 0.10603535920381546\n",
      "Epoch 32, Loss: 12.592829704284668\n",
      "Epoch 33, Loss: 0.4235636591911316\n",
      "Epoch 34, Loss: 5.4571380615234375\n",
      "Epoch 35, Loss: 0.05535614863038063\n",
      "Epoch 36, Loss: 6.776508808135986\n",
      "Epoch 37, Loss: 0.015035267919301987\n",
      "Epoch 38, Loss: 0.006586744915693998\n",
      "Epoch 39, Loss: 0.0010350811062380672\n",
      "Epoch 40, Loss: 0.003676976775750518\n",
      "Epoch 41, Loss: 0.7460736036300659\n",
      "Epoch 42, Loss: 0.032828446477651596\n",
      "Epoch 43, Loss: 0.010735253803431988\n",
      "Epoch 44, Loss: 0.0011918013915419579\n",
      "Epoch 45, Loss: 21.328956604003906\n",
      "Epoch 46, Loss: 0.03620041161775589\n",
      "Epoch 47, Loss: 0.023325061425566673\n",
      "Epoch 48, Loss: 0.05611748620867729\n",
      "Epoch 49, Loss: 6.869379043579102\n",
      "Epoch 50, Loss: 0.004680253099650145\n",
      "Epoch 51, Loss: 0.1589173525571823\n",
      "Epoch 52, Loss: 0.1691441535949707\n",
      "Epoch 53, Loss: 0.004418321419507265\n",
      "Epoch 54, Loss: 0.0843028873205185\n",
      "Epoch 55, Loss: 0.7778143882751465\n",
      "Epoch 56, Loss: 0.00038989336462691426\n",
      "Epoch 57, Loss: 0.002255750820040703\n",
      "Epoch 58, Loss: 0.00037787482142448425\n",
      "Epoch 59, Loss: 25.985748291015625\n",
      "Epoch 60, Loss: 0.0526064969599247\n",
      "Epoch 61, Loss: 0.0008483768324367702\n",
      "Epoch 62, Loss: 0.003982570953667164\n",
      "Epoch 63, Loss: 30.946706771850586\n",
      "Epoch 64, Loss: 0.0015124608762562275\n",
      "Epoch 65, Loss: 0.22007037699222565\n",
      "Epoch 66, Loss: 9.929485321044922\n",
      "Epoch 67, Loss: 1.3644685745239258\n",
      "Epoch 68, Loss: 0.021777242422103882\n",
      "Epoch 69, Loss: 0.01717766933143139\n",
      "Epoch 70, Loss: 24.378019332885742\n",
      "Epoch 71, Loss: 0.34202688932418823\n",
      "Epoch 72, Loss: 0.023625802248716354\n",
      "Epoch 73, Loss: 2.2979063987731934\n",
      "Epoch 74, Loss: 0.0016186560969799757\n",
      "Epoch 75, Loss: 0.0007299641147255898\n",
      "Epoch 76, Loss: 0.0003459069994278252\n",
      "Epoch 77, Loss: 2.3014235496520996\n",
      "Epoch 78, Loss: 0.944568395614624\n",
      "Epoch 79, Loss: 3.2232251167297363\n",
      "Epoch 80, Loss: 0.014051271602511406\n",
      "Epoch 81, Loss: 3.785613775253296\n",
      "Epoch 82, Loss: 0.016884872689843178\n",
      "Epoch 83, Loss: 0.04765206202864647\n",
      "Epoch 84, Loss: 7.256664752960205\n",
      "Epoch 85, Loss: 0.05549023300409317\n",
      "Epoch 86, Loss: 0.0005894891801290214\n",
      "Epoch 87, Loss: 0.00047356169670820236\n",
      "Epoch 88, Loss: 0.7622575163841248\n",
      "Epoch 89, Loss: 0.00298314169049263\n",
      "Epoch 90, Loss: 0.007418142165988684\n",
      "Epoch 91, Loss: 0.005388523451983929\n",
      "Epoch 92, Loss: 0.01809396967291832\n",
      "Epoch 93, Loss: 1.2337216138839722\n",
      "Epoch 94, Loss: 1.245928168296814\n",
      "Epoch 95, Loss: 8.71733283996582\n",
      "Epoch 96, Loss: 0.01445732545107603\n",
      "Epoch 97, Loss: 0.00032348610693588853\n",
      "Epoch 98, Loss: 0.0006532614352181554\n",
      "Epoch 99, Loss: 1.0719661712646484\n"
     ]
    }
   ],
   "source": [
    "# model definition and training\n",
    "\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(2, 10)\n",
    "        self.linear2 = torch.nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    # this method is necessary for the model to be evaluated by poly-lithic\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        # x will be a dicrt of keys and values\n",
    "        # {\"x\": x, \"y\": y}\n",
    "        input_tensor = torch.tensor([x['x'], x['y']], dtype=torch.float32)\n",
    "        # you may want to do somethinf more complex here\n",
    "        output_tensor = self.forward(input_tensor)\n",
    "        # return a dictionary of keys and values\n",
    "        return {\"max\": output_tensor.item()}        \n",
    "    \n",
    "# model training\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "batch_size = 32\n",
    "\n",
    "# convert data to torch tensors\n",
    "X = torch.tensor(df[['x', 'y']].values, dtype=torch.float32)\n",
    "y = torch.tensor(df[['max']].values, dtype=torch.float32)\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fedb6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3911.0959375\n",
      "Input: [-877.3532425   833.82588463], Predicted: 834.4656982421875, Actual: 833.8258666992188\n",
      "Input: [-440.90725107 -985.49221843], Predicted: -443.4677429199219, Actual: -440.9072570800781\n",
      "Input: [-133.4418616  -480.41190893], Predicted: -134.64442443847656, Actual: -133.44186401367188\n",
      "Input: [-466.10806235  774.32364276], Predicted: 775.1988525390625, Actual: 774.3236694335938\n",
      "Input: [-690.96579231  794.55989219], Predicted: 795.272216796875, Actual: 794.5598754882812\n",
      "Input: [-231.62883244  933.3487746 ], Predicted: 934.8477783203125, Actual: 933.3487548828125\n",
      "Input: [ 770.97776864 -222.97240719], Predicted: 771.0821533203125, Actual: 770.977783203125\n",
      "Input: [-636.76258515 -351.32397158], Predicted: -352.3260192871094, Actual: -351.323974609375\n",
      "Input: [-670.6501325  -553.75296158], Predicted: -555.0887451171875, Actual: -553.7529907226562\n",
      "Input: [-441.7944957    63.48388469], Predicted: 63.25518798828125, Actual: 63.483882904052734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gbm96348/nfs_home/ml_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_data = pd.DataFrame({\n",
    "    'x': np.random.uniform(-1000, 1000, 100),\n",
    "    'y': np.random.uniform(-1000, 1000, 100)\n",
    "})\n",
    "test_X = torch.tensor(test_data[['x', 'y']].values, dtype=torch.float32)\n",
    "test_y = torch.tensor(np.maximum(test_data['x'], test_data['y']), dtype=torch.float32)\n",
    "test_y_pred = model(test_X)\n",
    "test_loss = loss_fn(test_y_pred, test_y)\n",
    "print(f'Test Loss: {test_loss.item()/len(test_X)}')\n",
    "\n",
    "# print sample\n",
    "for i in range(10):\n",
    "    print(f\"Input: {test_data.iloc[i].values}, Predicted: {test_y_pred[i].item()}, Actual: {test_y[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b5966",
   "metadata": {},
   "source": [
    "# Stage 2 - Write a poly-lithic compatible model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9cc23",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# now we need to define the model wrapper and save it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
